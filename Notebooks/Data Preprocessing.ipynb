{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleted.greedy = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luisf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/1f3811a0-1fb9-4d43-974c-8c7bdc025d92/resourceGroups/disasterLocator/providers/Microsoft.MachineLearningServices/workspaces/disasterLocatorML',\n",
       " 'name': 'disasterLocatorML',\n",
       " 'location': 'eastus',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'tags': {},\n",
       " 'sku': 'Basic',\n",
       " 'workspaceid': '90beb71e-3575-4cbd-af2a-cb2c303edc7b',\n",
       " 'description': '',\n",
       " 'friendlyName': 'disasterLocatorML',\n",
       " 'creationTime': '2020-11-11T14:39:30.8601986+00:00',\n",
       " 'containerRegistry': '/subscriptions/1f3811a0-1fb9-4d43-974c-8c7bdc025d92/resourceGroups/disasterLocator/providers/Microsoft.ContainerRegistry/registries/90beb71e35754cbdaf2acb2c303edc7b',\n",
       " 'keyVault': '/subscriptions/1f3811a0-1fb9-4d43-974c-8c7bdc025d92/resourcegroups/disasterlocator/providers/microsoft.keyvault/vaults/disasterlocato9806975694',\n",
       " 'applicationInsights': '/subscriptions/1f3811a0-1fb9-4d43-974c-8c7bdc025d92/resourcegroups/disasterlocator/providers/microsoft.insights/components/disasterlocato5776021600',\n",
       " 'identityPrincipalId': 'ffd7eac5-c771-422b-8080-675c5d58b978',\n",
       " 'identityTenantId': 'b2cc71a7-45cf-49fe-b78c-a7d1913afbfb',\n",
       " 'identityType': 'SystemAssigned',\n",
       " 'storageAccount': '/subscriptions/1f3811a0-1fb9-4d43-974c-8c7bdc025d92/resourcegroups/disasterlocator/providers/microsoft.storage/storageaccounts/disasterlocato4377676546',\n",
       " 'hbiWorkspace': False,\n",
       " 'discoveryUrl': 'https://eastus.experiments.azureml.net/discovery',\n",
       " 'notebookInfo': {'fqdn': 'ml-disasterlocatorm-eastus-90beb71e-3575-4cbd-af2a-cb2c303edc7b.notebooks.azure.net',\n",
       "  'resource_id': 'f17d49a4f9de44f9b8500ef5fd322cd7'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Key Vault Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvault = ws.get_default_keyvault()\n",
    "subscription_id = keyvault.get_secret(\"subscription-id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach Compute Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target: nlp-cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "# compute cluster configuration\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"nlp-cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target: ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consume Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0  1.0    None     None  Our Deeds are the Reason of this #earthquake M...   \n",
       "1  4.0    None     None             Forest fire near La Ronge Sask. Canada   \n",
       "2  5.0    None     None  All residents asked to 'shelter in place' are ...   \n",
       "3  6.0    None     None  13,000 people receive #wildfires evacuation or...   \n",
       "4  7.0    None     None  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.get_by_name(ws, name='disaster_tweets_train')\n",
    "tweets_pd = dataset.to_pandas_dataframe()\n",
    "tweets_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...     1.0\n",
       "1             Forest fire near La Ronge Sask. Canada     1.0\n",
       "2  All residents asked to 'shelter in place' are ...     1.0\n",
       "3  13,000 people receive #wildfires evacuation or...     1.0\n",
       "4  Just got sent this photo from Ruby #Alaska as ...     1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pd.drop(columns=['id', 'keyword', 'location'], inplace=True)\n",
    "tweets_pd.dropna(inplace=True)\n",
    "tweets_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7176 entries, 0 to 8392\n",
      "Data columns (total 2 columns):\n",
      "text      7176 non-null object\n",
      "target    7176 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 168.2+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.429348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.495018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target\n",
       "count  7176.000000\n",
       "mean      0.429348\n",
       "std       0.495018\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Traning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x sample:  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "y sample:  1.0\n"
     ]
    }
   ],
   "source": [
    "x = tweets_pd['text'].values\n",
    "y = tweets_pd['target'].values\n",
    "print(\"x sample: \", x[0])\n",
    "print(\"y sample: \", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() \n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "def clean_tweet(text):\n",
    "    if type(text)!=str and type(text)!=np.str_:\n",
    "        print(\"type of \", type(text), \"cannot be processed\")\n",
    "        return\n",
    "    # remove special characters\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    # remove stopwords\n",
    "    cleaned_tweet = []\n",
    "    for word in text.split():\n",
    "        stem_word = stemmer.stem(word)\n",
    "        if stem_word not in stopwords_english:\n",
    "            cleaned_tweet.append(stem_word)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [clean_tweet(tweet) for tweet in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deed', 'reason', 'thi', 'earthquak', 'may', 'allah', 'forgiv', 'us']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tweets_array):\n",
    "    vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "    for tweet in tweets_array:\n",
    "        for word in tweet:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Tweets to Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_sequences(sentence_tweets, vocab, unk_tag='__UNK__'):\n",
    "    transformed_tweets = []\n",
    "    for tweet in sentence_tweets:\n",
    "        processed_tweet = []\n",
    "        for word in tweet:\n",
    "            if word in vocab:\n",
    "                processed_tweet.append(vocab[word])\n",
    "            else:\n",
    "                processed_tweet.append(vocab[unk_tag])\n",
    "        transformed_tweets.append(np.array(processed_tweet, dtype=np.int64))\n",
    "    return transformed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tweets_to_sequences(X_train, vocab, unk_tag='__UNK__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(tweet) for tweet in X_train])+1 #+1 for end of sentence tag \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, vocab, max_len, end_tag='__</e>__', pad_tag='__PAD__'):\n",
    "    padded_sequences = []\n",
    "    for sequence in sequences:\n",
    "        padded_sequence = np.array(list(sequence) + [vocab[end_tag]] + [vocab[pad_tag]]*(max_len-len(sequence)-1))\n",
    "        if (len(padded_sequence) == max_len):\n",
    "            padded_sequences.append(padded_sequence)\n",
    "    return np.array(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, vocab, max_len, end_tag='__</e>__', pad_tag='__PAD__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7176, 27)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])==max_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
